{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, gc\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score,\\\n",
    "f1_score, precision_score, recall_score, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options\n",
    "embed_ver = [\"clstm\", \"esm2\", \"bert\", \"t5\"]\n",
    "data_path = \"../data/test_exam/\"\n",
    "model_path = \"../models/classifier_indiv/\"\n",
    "result_path = \"../results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_str = ['file_id', 'organism', 'locus_tag', 'ess']\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data list for test dataset\n",
    "ts_data = {\n",
    "    \"data1\": [\"C018\"],  # \"Escherichia coli K-12 BW25113\"\n",
    "    \"data2\": [\"C016\"],  # \"Escherichia coli K-12 MG1655\"\n",
    "    \"data3\": [\"O046\"],  # \"synthetic bacterium JCVI-Syn3A\"\n",
    "    \"data4\": [\"C048\"],  # Bacteroides thetaiotaomicron VPI-5482\n",
    "    \"data5\": [\"C050\"]  # Salmonella enterica subsp. enterica serovar Typhimurium str. 14028S\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to record perfomance result\n",
    "def record_perform(emb_ver, file_id, organ, y_real, y_conf, y_prd):\n",
    "    y_real = y_real.cpu().numpy()\n",
    "    y_conf = y_conf.cpu().numpy()\n",
    "    y_prd = y_prd.cpu().numpy()\n",
    "    \n",
    "    if file_id != \"O046\":\n",
    "        auc_roc = [roc_auc_score(y_real, y_conf)]\n",
    "        auc_pr = [average_precision_score(y_real, y_conf)]\n",
    "    else:\n",
    "        auc_roc = None\n",
    "        auc_pr = None\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_real, y_prd).ravel()\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        \"emb\": [emb_ver],\n",
    "        \"file\": [file_id],\n",
    "        \"organism\": [organ],\n",
    "        \"tp\": [tp],\n",
    "        \"fp\": [fp],\n",
    "        \"tn\": [tn],\n",
    "        \"fn\": [fn],\n",
    "        \"mcc\": [matthews_corrcoef(y_real, y_prd)],\n",
    "        \"acc\": [accuracy_score(y_real, y_prd)],\n",
    "        \"f1\": [f1_score(y_real, y_prd)],\n",
    "        \"prc\": [precision_score(y_real, y_prd)],\n",
    "        \"rec\": [recall_score(y_real, y_prd)],\n",
    "        \"npv\": [precision_score(1 - y_real, 1 - y_prd)],\n",
    "        \"tnr\": [recall_score(1 - y_real, 1 - y_prd)],\n",
    "        \"auc-roc\": auc_roc,\n",
    "        \"auc-pr\": auc_pr\n",
    "    })\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model architecture\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, unit_decrease):\n",
    "        super(Classifier, self).__init__()\n",
    "        layers = [nn.BatchNorm1d(input_size), nn.Dropout(0.5)]\n",
    "        in_dim = input_size\n",
    "        out_dim = 1024\n",
    "        for i in range(num_layers):            \n",
    "            out_dim = max(2, out_dim // unit_decrease)\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            self.initialize_weights(layers[-1])\n",
    "            layers.append(nn.GELU())\n",
    "            in_dim = out_dim\n",
    "        layers.append(nn.Linear(out_dim, 1))\n",
    "        self.cls_block = nn.Sequential(*layers)\n",
    "        \n",
    "    def initialize_weights(self, layer):\n",
    "        nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='linear')\n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.cls_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame()\n",
    "\n",
    "for ver in embed_ver:\n",
    "    model_name = f'cls-{ver}'\n",
    "    print(f\"\\n>>>> {model_name} <<<<\")    \n",
    "    \n",
    "    # load dataset\n",
    "    data = pd.read_csv(data_path + f\"data_emb-{ver}.csv\")\n",
    "    display(\"Raw data:\", data)\n",
    "\n",
    "    #### Preprocess for test dataset ####\n",
    "    col_num = [col for col in data.columns if col not in col_str]\n",
    "\n",
    "    # get test datasets\n",
    "    loc_ts = {}\n",
    "    data_ts = {}\n",
    "    org_ts = {}\n",
    "    for ts_ver, ids in ts_data.items():\n",
    "        # get test sample locations\n",
    "        loc_ts[ts_ver] = data['file_id'].isin(ids)\n",
    "        # get test samples\n",
    "        data_ts[ts_ver] = data[loc_ts[ts_ver]]\n",
    "        org = []\n",
    "        # get test organism list\n",
    "        for i in ids:\n",
    "            organ = data_ts[ts_ver]['organism'][data_ts[ts_ver]['file_id'] == i].to_list()\n",
    "            if len(organ) > 0:\n",
    "                org.append(organ[0])\n",
    "        org_ts[ts_ver] = org\n",
    "\n",
    "        print(\"Test dataset(\" + ts_ver + \"):\", data_ts[ts_ver].shape)\n",
    "    print(\"Test organism:\", org_ts, len(org_ts))\n",
    "\n",
    "    # split info.& inputs & labels of the test datasets\n",
    "    info_ts = {}\n",
    "    y_ts = {}\n",
    "    test_loader = {}\n",
    "    for ts_ver, df in data_ts.items():\n",
    "        info_ts[ts_ver] = df[col_str]\n",
    "        X_ts = torch.tensor(df[col_num].astype('float32').values)\n",
    "        y_ts[ts_ver] = torch.tensor(df['ess'].astype('float32').values)\n",
    "        print(\"Splited test dataset(\" + ts_ver + \"):\", X_ts.shape, y_ts[ts_ver].shape)                    \n",
    "        # generate dataloader by the test datasets\n",
    "        dataset_ts = TensorDataset(X_ts, y_ts[ts_ver])\n",
    "        test_loader[ts_ver] = DataLoader(dataset_ts, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # get the total test dataset\n",
    "    loc_ts_all = [sum(loc) >= 1 for loc in zip(*loc_ts.values())]\n",
    "    info_ts_all = data.loc[loc_ts_all, col_str]\n",
    "    X_ts_all = torch.tensor(data.loc[loc_ts_all, col_num].astype('float32').values)\n",
    "    y_ts_all = torch.tensor(data.loc[loc_ts_all, 'ess'].astype('float32').values)\n",
    "    \n",
    "    print(\"Splited test dataset(all):\", X_ts_all.shape, y_ts_all.shape)\n",
    "\n",
    "    # generate dataloader of total test dataset\n",
    "    test_all_dataset = TensorDataset(X_ts_all, y_ts_all)\n",
    "    test_all_loader = DataLoader(test_all_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "\n",
    "    #### Evaluate model ####\n",
    "    \n",
    "    # generate model instance\n",
    "    model = Classifier(\n",
    "        input_size=X_ts_all.shape[-1],\n",
    "        num_layers=3,\n",
    "        unit_decrease=2\n",
    "    ).to(device)\n",
    "\n",
    "    # load model weight\n",
    "    model.load_state_dict(torch.load(model_path + model_name + \".pt\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    ## model evaluations by test dataset ##\n",
    "    for ts_ver, ids in ts_data.items():\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader[ts_ver]:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                preds = model(X_batch).squeeze()\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(y_batch.cpu())\n",
    "        \n",
    "        # concatenate results to one tensor\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "\n",
    "        # convert logits to confidences & classes\n",
    "        prd_conf = torch.sigmoid(all_preds)\n",
    "        prd_cls = (prd_conf >= 0.5).int()\n",
    "        # performances by testset\n",
    "        perform = record_perform(\n",
    "            emb_ver=ver,\n",
    "            file_id=\"+\".join(ids),\n",
    "            organ=\"+\".join(org_ts[ts_ver]),\n",
    "            y_real=y_ts[ts_ver],\n",
    "            y_conf=prd_conf,\n",
    "            y_prd=prd_cls,\n",
    "        )\n",
    "        df_eval = pd.concat([df_eval, perform], ignore_index=True)\n",
    "        print(f\"- Test in {ts_ver} was done.\")\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    ## model evaluation on the total test dataset ##\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_all_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = model(X_batch).squeeze()\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(y_batch.cpu())\n",
    "    \n",
    "    # concatenate results to one tensor\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # convert logits to confidences & classes\n",
    "    prd_conf = torch.sigmoid(all_preds)\n",
    "    prd_cls = (prd_conf >= 0.5).int()\n",
    "\n",
    "    # performances on total testset\n",
    "    perform = record_perform(\n",
    "        emb_ver=ver,\n",
    "        file_id=\"total\",\n",
    "        organ=\"all\",\n",
    "        y_real=y_ts_all,\n",
    "        y_conf=prd_conf,\n",
    "        y_prd=prd_cls\n",
    "    )\n",
    "    df_eval = pd.concat([df_eval, perform], ignore_index=True)\n",
    "    print(f\"- Test in total testset was done.\")\n",
    "\n",
    "    # concatenate the protein info. & predicted confidences\n",
    "    df_prd = pd.DataFrame(prd_conf, columns=[\"conf\"], index=info_ts_all.index)\n",
    "    df_prd = pd.concat([info_ts_all, df_prd], axis=1)\n",
    "\n",
    "    # save the model prediction result\n",
    "    df_prd.to_csv(f\"{result_path}prd-indiv_class/{model_name}.csv\", index=False)\n",
    "    \n",
    "    print(f\"- Prediction by {model_name} was done.\")\n",
    "\n",
    "# save the model perfomance result\n",
    "display(\"Model performance:\", df_eval)\n",
    "df_eval.to_csv(f\"{result_path}eval-indiv_class.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
